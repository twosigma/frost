#!/usr/bin/env python3

#    Copyright 2026 Two Sigma Open Source, LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License");
#    you may not use this file except in compliance with the License.
#    You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS,
#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    See the License for the specific language governing permissions and
#    limitations under the License.

"""Run riscv-torture tests on Frost.

Signature-based verification: compiles adapted torture tests, runs cocotb
simulation, extracts UART signature, and compares against Spike golden
references.

Can be run standalone:
    ./test_riscv_torture.py --sim verilator --all
    ./test_riscv_torture.py --sim verilator --test test_001
    ./test_riscv_torture.py --sim verilator --parallel 4

Or via pytest:
    pytest test_riscv_torture.py -v --sim verilator -m slow
"""

import argparse
import os
import subprocess
import sys
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import pytest

from test_run_cocotb import CocotbRunner

# Directory layout
TESTS_DIR = Path(__file__).parent.resolve()
REPO_ROOT = TESTS_DIR.parent
TORTURE_APP_DIR = REPO_ROOT / "sw" / "apps" / "riscv_torture"
TORTURE_TESTS_DIR = TORTURE_APP_DIR / "tests"
REFERENCES_DIR = TORTURE_APP_DIR / "references"


@dataclass
class TestResult:
    """Result of a single torture test."""

    test_name: str
    status: str  # "PASS", "FAIL", "SKIP"
    message: str = ""


def discover_tests() -> list[Path]:
    """Find all adapted .S test files."""
    if not TORTURE_TESTS_DIR.is_dir():
        return []
    return sorted(TORTURE_TESTS_DIR.glob("*.S"))


def get_reference_path(test_src: Path) -> Path:
    """Get the golden reference output path for a test source file."""
    return REFERENCES_DIR / f"{test_src.stem}.reference_output"


def compile_test(test_src: Path) -> bool:
    """Compile a single torture test, returns True on success."""
    subprocess.run(
        ["make", "clean"],
        cwd=TORTURE_APP_DIR,
        capture_output=True,
        text=True,
        timeout=30,
    )

    rel_src = test_src.relative_to(TORTURE_APP_DIR)
    result = subprocess.run(
        ["make", f"TEST_SRC={rel_src}"],
        cwd=TORTURE_APP_DIR,
        capture_output=True,
        text=True,
        timeout=120,
    )
    return result.returncode == 0


def run_simulation(simulator: str) -> subprocess.CompletedProcess[str] | None:
    """Run cocotb simulation and return the result."""
    runner = CocotbRunner(
        python_test_module="cocotb_tests.test_real_program",
        hdl_toplevel_module="frost",
        app_name=None,
    )

    os.environ["SIM"] = simulator
    env = runner.setup_environment()
    sim_build_dir = runner._get_sim_build_dir(env)
    env["SIM_BUILD"] = str(sim_build_dir)
    env["COCOTB_MAX_CYCLES"] = "10000000"

    original_dir = os.getcwd()
    os.chdir(TESTS_DIR)

    try:
        needs_clean = simulator != "verilator" or runner._verilator_needs_rebuild(
            sim_build_dir
        )
        if needs_clean:
            subprocess.run(["make", "clean"], check=False)

        sw_mem_path = Path("sw.mem")
        if sw_mem_path.exists() or sw_mem_path.is_symlink():
            sw_mem_path.unlink()
        sw_mem_target = TORTURE_APP_DIR / "sw.mem"
        sw_mem_path.symlink_to(sw_mem_target)

        pythonpath = env.get("PYTHONPATH", "")
        cmd = (
            f"export PYTHONPATH='{pythonpath}' && "
            f"make COCOTB_TEST_MODULES='cocotb_tests.test_real_program' "
            f"TOPLEVEL=frost"
        )
        result = subprocess.run(
            ["bash", "-c", cmd],
            capture_output=True,
            text=True,
            env=env,
            check=False,
            timeout=7200,
        )

        if simulator == "verilator" and result.returncode == 0:
            runner._update_verilator_toplevel_marker(sim_build_dir)

        return result

    except subprocess.TimeoutExpired:
        return None
    finally:
        sw_mem_path = Path("sw.mem")
        if sw_mem_path.exists() or sw_mem_path.is_symlink():
            sw_mem_path.unlink()
        os.chdir(original_dir)


def extract_signature(sim_output: str) -> list[str]:
    """Extract hex signature lines from simulation UART output.

    Same logic as arch_test: collect 8-char hex lines before <<PASS>>.
    """
    lines = sim_output.splitlines()
    sig_lines: list[str] = []
    collecting = False
    for line in lines:
        stripped = line.strip()
        if len(stripped) == 8 and all(c in "0123456789abcdefABCDEF" for c in stripped):
            collecting = True
            sig_lines.append(stripped.lower())
        elif collecting and stripped.startswith("<<PASS>>"):
            break
        elif collecting and stripped:
            sig_lines = []
            collecting = False
    return sig_lines


def load_reference(ref_path: Path) -> list[str]:
    """Load golden reference signature file."""
    lines = []
    for line in ref_path.read_text().splitlines():
        stripped = line.strip()
        if stripped:
            lines.append(stripped.lower())
    return lines


# Integer register indices (in the 32-word GPR section) that may contain
# address-derived values differing between Frost and Spike memory layouts:
#   x2 (sp), x3 (gp), x5 (t0 â€” clobbered by footer), x31 (mem base).
# AMO sequences also compute addresses into random GPRs via
#   addi addr_reg, x31, offset
# making those words unpredictable across layouts.  We therefore compare
# only the FP portion (words 32-95) for cross-platform correctness and
# accept the integer portion as long as the word count matches.
_SKIP_GPR_COMPARISON = True
_GPR_WORDS = 32  # x0-x31
_TOTAL_WORDS = 96  # 32 GPR + 64 FP (32 doubles * 2 words)


def compare_signatures(actual: list[str], expected: list[str]) -> tuple[bool, str]:
    """Compare actual vs expected signatures.

    Compares FP register words exactly.  Integer register words are
    skipped because AMO address computation contaminates random GPRs
    with layout-dependent addresses.
    """
    if len(actual) != _TOTAL_WORDS:
        return (
            False,
            f"actual signature has {len(actual)} words, expected {_TOTAL_WORDS}",
        )
    if len(expected) != _TOTAL_WORDS:
        return False, f"reference has {len(expected)} words, expected {_TOTAL_WORDS}"

    # Compare FP words (index 32..95)
    diff_lines = []
    start = _GPR_WORDS if _SKIP_GPR_COMPARISON else 0
    for i in range(start, _TOTAL_WORDS):
        act = actual[i]
        exp = expected[i]
        if act != exp:
            diff_lines.append(f"  word {i}: got {act}, expected {exp}")
            if len(diff_lines) >= 10:
                diff_lines.append("  ... and more")
                break

    if not diff_lines:
        return True, ""
    return False, "\n".join(diff_lines)


def run_single_test(test_src: Path, simulator: str) -> TestResult:
    """Build, simulate, and verify a single torture test."""
    test_name = test_src.stem

    ref_path = get_reference_path(test_src)
    if not ref_path.exists():
        return TestResult(test_name, "SKIP", "No reference output")

    if not compile_test(test_src):
        return TestResult(test_name, "SKIP", "Compilation failed")

    result = run_simulation(simulator)
    if result is None:
        return TestResult(test_name, "SKIP", "Simulation timed out")

    combined_output = (result.stdout or "") + (result.stderr or "")

    if result.returncode != 0:
        return TestResult(test_name, "SKIP", "Simulation error")

    if "<<FAIL>>" in combined_output:
        return TestResult(test_name, "FAIL", "Test trapped (<<FAIL>> TRAP)")

    if "<<PASS>>" not in combined_output:
        return TestResult(test_name, "FAIL", "No <<PASS>> marker in output")

    actual_sig = extract_signature(combined_output)
    if not actual_sig:
        return TestResult(test_name, "FAIL", "No signature data in output")

    expected_sig = load_reference(ref_path)
    match, diff_msg = compare_signatures(actual_sig, expected_sig)

    if match:
        return TestResult(test_name, "PASS")
    else:
        return TestResult(
            test_name,
            "FAIL",
            f"Signature mismatch ({len(actual_sig)} actual vs {len(expected_sig)} expected words):\n{diff_msg}",
        )


def _run_test_worker(args: tuple[str, str, str]) -> TestResult:
    """Worker function for parallel test execution."""
    test_src_str, simulator, app_dir_str = args
    global TORTURE_APP_DIR, TORTURE_TESTS_DIR, REFERENCES_DIR
    TORTURE_APP_DIR = Path(app_dir_str)
    TORTURE_TESTS_DIR = TORTURE_APP_DIR / "tests"
    REFERENCES_DIR = TORTURE_APP_DIR / "references"

    return run_single_test(Path(test_src_str), simulator)


def _print_result(result: TestResult) -> None:
    """Print a single test result."""
    status_str = {"PASS": "PASS", "FAIL": "FAIL", "SKIP": "SKIP"}[result.status]
    line = f"  {result.test_name:40s} {status_str}"
    if result.message and result.status != "PASS":
        first_line = result.message.split("\n")[0]
        line += f"  ({first_line})"
    print(line)


def run_all_tests(
    simulator: str,
    parallel: int = 1,
) -> list[TestResult]:
    """Run all torture tests."""
    tests = discover_tests()
    if not tests:
        print(f"  No torture tests found in {TORTURE_TESTS_DIR}")
        return []

    print(f"\nriscv-torture ({len(tests)} tests)")

    results = []

    if parallel > 1:
        work_items = [(str(t), simulator, str(TORTURE_APP_DIR)) for t in tests]
        with ProcessPoolExecutor(max_workers=parallel) as executor:
            futures = {
                executor.submit(_run_test_worker, item): item[0] for item in work_items
            }
            for future in as_completed(futures):
                try:
                    result = future.result()
                except Exception as e:
                    failed_src = futures[future]
                    test_name = Path(failed_src).stem
                    result = TestResult(test_name, "SKIP", str(e))
                results.append(result)
                _print_result(result)
    else:
        for test_src in tests:
            result = run_single_test(test_src, simulator)
            results.append(result)
            _print_result(result)

    return results


# =============================================================================
# Pytest Integration
# =============================================================================


@pytest.mark.cocotb
@pytest.mark.slow
class TestRiscvTorture:
    """riscv-torture random instruction tests."""

    def test_riscv_torture(self, request: Any, capsys: Any) -> None:
        """Run all riscv-torture tests.

        Verilator only.
        """
        sim = request.config.getoption("--sim")
        if sim != "verilator":
            pytest.skip("riscv-torture tests require verilator")

        tests = discover_tests()
        if not tests:
            pytest.skip("No torture tests found (generate with generate_tests.py)")

        os.environ["SIM"] = "verilator"
        with capsys.disabled():
            print("\nRunning riscv-torture tests...")
            results = run_all_tests("verilator")

        failed = [r for r in results if r.status == "FAIL"]
        if failed:
            msg = "\n".join(f"  {r.test_name}: {r.message}" for r in failed)
            pytest.fail(f"{len(failed)} torture test(s) failed:\n{msg}")


# =============================================================================
# Standalone CLI
# =============================================================================


def main() -> int:
    """Run riscv-torture tests on Frost."""
    parser = argparse.ArgumentParser(
        description="Run riscv-torture tests on Frost",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --sim verilator --all
  %(prog)s --sim verilator --test test_001
  %(prog)s --sim verilator --parallel 4
""",
    )
    parser.add_argument(
        "--sim",
        required=True,
        choices=["icarus", "verilator"],
        help="Simulator to use",
    )
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "--all",
        action="store_true",
        help="Run all torture tests",
    )
    group.add_argument(
        "--test",
        metavar="NAME",
        help="Run a single test by name (without .S extension)",
    )
    group.add_argument(
        "--list",
        action="store_true",
        help="List available torture tests",
    )
    parser.add_argument(
        "--parallel",
        type=int,
        default=1,
        metavar="N",
        help="Number of parallel test workers (default: 1)",
    )

    args = parser.parse_args()

    if args.list:
        tests = discover_tests()
        if not tests:
            print(f"No torture tests found in {TORTURE_TESTS_DIR}/")
            return 0
        print(f"Torture tests ({len(tests)}):")
        for t in tests:
            ref = get_reference_path(t)
            ref_status = "ref" if ref.exists() else "NO REF"
            print(f"  {t.stem:40s} [{ref_status}]")
        return 0

    if args.test:
        test_path = TORTURE_TESTS_DIR / f"{args.test}.S"
        if not test_path.exists():
            print(f"Error: Test not found: {test_path}")
            return 1

        print(f"=== riscv-torture: {args.test} ===")
        result = run_single_test(test_path, args.sim)
        _print_result(result)
        return 0 if result.status == "PASS" else 1

    # All tests mode
    print("=" * 60)
    print("riscv-torture Test Results")
    print(f"Simulator: {args.sim}")
    print("=" * 60)

    all_results = run_all_tests(args.sim, parallel=args.parallel)

    n_pass = sum(1 for r in all_results if r.status == "PASS")
    n_fail = sum(1 for r in all_results if r.status == "FAIL")
    n_skip = sum(1 for r in all_results if r.status == "SKIP")

    print()
    print("=" * 60)
    print(f"Summary: {n_pass} PASS, {n_fail} FAIL, {n_skip} SKIP")
    print("=" * 60)

    failed = [r for r in all_results if r.status == "FAIL"]
    if failed:
        print("\nFailed tests:")
        for r in failed:
            print(f"  {r.test_name}")
            if r.message:
                for line in r.message.split("\n"):
                    print(f"    {line}")

    return 1 if n_fail > 0 else 0


if __name__ == "__main__":
    sys.exit(main())
